---
title: "Linear Regression Formulas"
output:
  pdf_document:
    template: template.tex
linkcolor: black
fontsize: 11pt
lang: en-AU
---

# Linear Regression Formulas

 *Linear Regression Equation* $y = \alpha + \beta x + \epsilon$  
 
 
 *Matrix form:* $\pmb{Y} = \pmb{\beta} \pmb{X} + \pmb{\epsilon}$ 
 
  with $E(Y) = \pmb{\beta} X$
  


## Acronyms and names

* *TSS*: Total Sum of Squares
* *RSS*: Residual Sum of Squares
* *MSS*: Mean Sum of Squares
* $S_xx$: Corrected sum of squares of x
* $S_yy$: Corrected sum of squares of y
* $S_xy$: Corrected sum of products of xy
* *n* : number of observations
* *p* : number of parameters (does not include intercept)

## Least square estimates (matrix form)

Sum of squares function
$$
S(\pmb{\beta}) = \sum_{i=1}^{n} y_i + X_i \pmb{\beta}
$$

### Estimates, matrix from

\begin{align*}
\widehat{\pmb{\beta}} &= (\pmb{X^TX})^{-1}\pmb{X^T Y} \\
RSS  &= S(\widehat{\pmb{\beta}}) = \pmb{Y^TY} - \pmb{Y^TX\widehat{\beta}} \\
\sigma^2 &= \frac{RSS}{n-p}
\end{align*}

where
$\sigma^2$ is the *variance*.
$n-p$ is the *degrees of freedom*.


### Estimates, non-matrix form

\begin{align*}
S_{xx} &= \sum_{i=1}^{n} (x_i - \overline{x})^2 \\
S_{yy} &= \sum_{i=1}^{n} (i_i - \overline{y})^2 \\
S_{xy} &= \sum_{i=1}^{n} (x_i - \overline{x})(i_i - \overline{y})  \\
\widehat{\beta} &= \frac{S_{xy}}{S_{xx}} \\
\widehat{\alpha} &= \overline{y} - \frac{S_{xy}}{S{xx}}\overline{x} 
\end{align*}

## Correlation

\begin{align*}
\rho(X,Y) &= \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} \\
r &= \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} \\
\end{align*}

## RSS,TSS, MSS

\begin{align*}
\widehat{\beta_0} &= \overline{y} \\
then \\
S(\widehat{\beta_0}) &= TSS \\
                     &= S_{yy} \\
                     &= \sum_{i=1}^{n}(y_i -\overline{y})^2
\end{align*}

\begin{align*}
RSS &= \sum_{i=1}^{n}(y_i -\widehat{y})^2
\end{align*}

\begin{align*}
TSS = MSS + RSS
\end{align*}


## $R^2$ (standard and adjusted)

\begin{align*}
R^2 &= 1 - \frac{RSS}{TSS}
\end{align*}

In the case of a *simple* linear regression (one explanatory variable) $R^2=r^2$.

\begin{align*}
R^2(adj) &= 1 - \frac{\frac{RSS}{n-p-1}}{\frac{TSS}{n-1}} \\
R^2(adj) &= 1 - (1- R^2)\frac{n-1}{n-p-1}
\end{align*}

## Assumptions of a linear model

* **A**: the deterministic part of the model captures all the non-random structure in the data.
* **B**: the scale of the variability of the errors is constant at all values of the explanatory variables.
* **C**: errors are independent.
* **D**: errors are normally distributed.
* **E**: the values of the explanatory variables are recorded without error.

## Residuals

\begin{align*}
\widehat{\epsilon_i} &= y_i - \widehat(y_i)
\end{align*}

*standardised residuals*

\begin{align*}
r_i &= \frac{\widehat{\epsilon_i}}{\sqrt{Var(\widehat{\epsilon_i})}}
\end{align*}

## Inference for Regression Coefficients

estimated standard error (e.s.e./s.e.)

\begin{align*}
e.s.e.(\pmb{\widehat{\beta}})&= e.s.e(\pmb{\widehat{\beta}}) = \sqrt{\frac{RSS}{n-p}\pmb{b}^T(\pmb{X}^T\pmb{X})^{-1} \pmb{b}}
\end{align*}


pivotal function

\begin{align*}
\frac{\pmb{b}^T \pmb{\widehat{\beta}} - \pmb{b}^T \pmb{\beta}}{e.s.e(\widehat{\beta})} \sim t (n-p; \frac{1+c}{2})
\end{align*}

with $c$ : confidence level ($c=0.95 \implies (c+1)/2=0.975$)

prediction interval

\begin{align*}
\pmb{b}^T\pmb{\widehat{\beta}} \pm t (n-p; \frac{1+c}{2}) e.s.e.(\pmb{\widehat{\beta}}) \\
\pmb{b}^T\pmb{\widehat{\beta}} \pm t (n-p; \frac{1+c}{2}) \sqrt{\frac{RSS}{n-p}\pmb{b}^T(\pmb{X}^T\pmb{X})^{-1} \pmb{b}}
\end{align*}

## ANOVA table

\tablehead{
\toprule
Component&df&SS&MS&F value\\
\midrule
}
\begin{xtabular}{lllll}
%\toprule
%Symbol&Description&Unit\\
%\midrule
Model       & $p-1$              & MSS            &$\frac{MSS}{p-1}$    &$\frac{\frac{MSS}{p-1}}{\frac{RSS}{n-p}}$ \\
Residual    & $n-p$              & RSS            &$\frac{RSS}{n-p}$    &     -                                  \\
Total       & $n-1$              & TSS            &   -                 &     -                                 
\end{xtabular}

* df : degrees of freedom
* SS : Sum of Squares
* MS : Mean Squares

F statistic ($H_0$ : all parameters = 0)
(large F values $\implies$ rejection of $H_0$)







