---
title: "Linear Regression Formulas"
output:
  pdf_document:
    template: template.tex
linkcolor: black
fontsize: 11pt
lang: en-AU
---

# Linear Regression Formulas

 *Linear Regression Equation* $y = \alpha + \beta x + \epsilon$  
 
 
 *Matrix form:* $\pmb{Y} = \pmb{\beta} \pmb{X} + \pmb{\epsilon}$ 
 
  with $E(Y) = \pmb{\beta} X$
  


## Acronyms and names

* *TSS*: Total Sum of Squares
* *RSS*: Residual Sum of Squares
* *MSS*: Mean Sum of Squares
* $S_xx$: Corrected sum of squares of x
* $S_yy$: Corrected sum of squares of y
* $S_xy$: Corrected sum of products of xy
* *n* : number of observations
* *p* : number of parameters (does not include intercept)

## Least square estimates (matrix form)

Sum of squares function
$$
S(\pmb{\beta}) = \sum_{i=1}^{n} y_i + X_i \pmb{\beta}
$$

### Estimates, matrix from

\begin{align*}
\widehat{\pmb{\beta}} &= (\pmb{X^TX})^{-1}\pmb{X^T Y} \\
RSS  &= S(\widehat{\pmb{\beta}}) = \pmb{Y^TY} - \pmb{Y^TX\widehat{\beta}} \\
\sigma^2 &= \frac{RSS}{n-p}
\end{align*}

where
$\sigma^2$ is the *variance*.
$n-p$ is the *degrees of freedom*.


### Estimates, non-matrix form

\begin{align*}
S_{xx} &= \sum_{i=1}^{n} (x_i - \overline{x})^2 \\
S_{yy} &= \sum_{i=1}^{n} (i_i - \overline{y})^2 \\
S_{xy} &= \sum_{i=1}^{n} (x_i - \overline{x})(i_i - \overline{y})  \\
\widehat{\beta} &= \frac{S_{xy}}{S_{xx}} \\
\widehat{\alpha} &= \overline{y} - \frac{S_{xy}}{S{xx}}\overline{x} 
\end{align*}

## Correlation

\begin{align*}
\rho(X,Y) &= \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} \\
r &= \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} \\
\end{align*}

## RSS,TSS, MSS

\begin{align*}
\widehat{\beta_0} &= \overline{y} \\
then \\
S(\widehat{\beta_0}) &= TSS \\
                     &= S_{yy} \\
                     &= \sum_{i=1}^{n}(y_i -\overline{y})^2
\end{align*}

\begin{align*}
RSS &= \sum_{i=1}^{n}(y_i -\widehat{y})^2
\end{align*}

\begin{align*}
TSS = MSS + RSS
\end{align*}


## $R^2$ (standard and adjusted)

\begin{align*}
R^2 &= 1 - \frac{RSS}{TSS}
\end{align*}

In the case of a *simple* linear regression (one explanatory variable) $R^2=r^2$.

\begin{align*}
R^2(adj) &= 1 - \frac{\frac{RSS}{n-p-1}}{\frac{TSS}{n-1}} \\
R^2(adj) &= 1 - (1- R^2)\frac{n-1}{n-p-1}
\end{align*}

## Assumptions of a linear model

* **A**: the deterministic part of the model captures all the non-random structure in the data.
* **B**: the scale of the variability of the errors is constant at all values of the explanatory variables.
* **C**: errors are independent.
* **D**: errors are normally distributed.
* **E**: the values of the explanatory variables are recorded without error.

## Residuals

\begin{align*}
\widehat{\epsilon_i} &= y_i - \widehat(y_i)
\end{align*}

*standardised residuals*

\begin{align*}
r_i &= \frac{\widehat{\epsilon_i}}{\sqrt{Var(\widehat{\epsilon_i})}}
\end{align*}

## Inference for Regression Coefficients

estimated standard error (e.s.e./s.e.)

\begin{align*}
e.s.e.(\pmb{\widehat{\beta}})&= e.s.e(\pmb{\widehat{\beta}}) = \sqrt{\frac{RSS}{n-p}\pmb{b}^T(\pmb{X}^T\pmb{X})^{-1} \pmb{b}}
\end{align*}


pivotal function

\begin{align*}
\frac{\pmb{b}^T \pmb{\widehat{\beta}} - \pmb{b}^T \pmb{\beta}}{e.s.e(\widehat{\beta})} \sim t (n-p; \frac{1+c}{2})
\end{align*}

with $c$ : confidence level ($c=0.95 \implies (c+1)/2=0.975$)

confidence interval (for slope parametres)

\begin{align*}
\pmb{b}^T\pmb{\widehat{\beta}} \pm t (n-p; \frac{1+c}{2}) e.s.e.(\pmb{\widehat{\beta}}) \\
\pmb{b}^T\pmb{\widehat{\beta}} \pm t (n-p; \frac{1+c}{2}) \sqrt{\frac{RSS}{n-p}(\pmb{b}^T(\pmb{X}^T\pmb{X})^{-1} \pmb{b})}
\end{align*}

prediction interval (for predicted variable)

\begin{align*}
\pmb{b}^T\pmb{\widehat{\beta}} \pm t (n-p; \frac{1+c}{2}) \sqrt{\frac{RSS}{n-p}(\textcolor{red}{1} + \pmb{b}^T(\pmb{X}^T\pmb{X})^{-1} \pmb{b})}
\end{align*}


## Different/Paralell  lines model
\\
model : $Y_{ij} = \alpha_i + \beta_i (x_{ij} + \overline{x}_i) + \epsilon_{ij}$ \\
into matrix notation : $\pmb{Y} = \pmb{X}\pmb{\beta} +\pmb{epsilon}$ \\

## Different lines

\begin{align*}
\pmb{Y} = \begin{pmatrix}
           y_{11} \\
           \vdots  \\
           y_{1n_1} \\
           y_{21}
           \vdots \\
           y_{2n_2}
          \end{pmatrix},
\pmb{X} = \begin{pmatrix}
          1 & (x_{11}-\overline{x_1}) & 0 & 0 \\
          \vdots & \vdots & \vdots & \vdots \\
          1 & (x_{1n_1}-\overline{x_1}) & 0 & 0 \\
          0 & 0-\overline{x}_1) & 1 &  (x_{21}-\overline{x}_2) \\
          0 & 0-\overline{x}_1) & 1 &  (x_{2n_2}-\overline{x}_2) 
          \end{pmatrix},
\pmb{\beta} = \begin{pmatrix}
              \alpha_1 \\
              \beta_1 \\
              \alpha_2 \\
              \beta_2 
              \end{pmatrix}
\end{align*}

Least squares estimate

\begin{align*}
\pmb{\widehat{\beta}} &= \begin{pmatrix}
                        \overline{y}_1 \\
                        \frac{S_{x_1y_1}}{S_{x_1x_1}}  \\
                        \overline{y}_2 \\
                        \frac{S_{x_2y_2}}{S_{x_2x_2}}  \\
                      \end{pmatrix} \\
RSS &= RSS1 + RSS2                      
\end{align*}

95% confidence interval for ($\beta_1 - \beta_2$)

\begin{align*}
\widehat{\beta}_1 - \widehat{\beta}_2 \pm t(n_1 + n_2 - 4; 0.975 ) \sqrt{\frac{RSS_1 + RSS_2}{n_1 + n_2 -4}(\frac{1}{S_{x_1x_2}}+\frac{1}{S_{x_1x_2}})}
\end{align*}

## Parallel lines

\begin{align*}
\pmb{Y} = \begin{pmatrix}
           y_{11} \\
           \vdots  \\
           y_{1n_1} \\
           y_{21}
           \vdots \\
           y_{2n_2}
          \end{pmatrix},
\pmb{X} = \begin{pmatrix}
            1 & 0 & (x_{11} - \overline{x}_1) \\
            \vdots & \vdots & \vdots \\
            1 & 0 & (x_{1n_1} - \overline{x}_1) \\
            0 & 1 & (x_{21} - \overline{x}_2) \\
            \vdots & \vdots & \vdots \\
            0 & 1 & (x_{2n_2} - \overline{x}_2)
          \end{pmatrix},
\pmb{\beta} =\begin{pmatrix}
              \alpha_1 \\
              \alpha_2 \\
              \beta
              \end{pmatrix}
\end{align*}

\begin{align*}
\pmb{\widehat{\beta}} = \begin{pmatrix}
                        \overline{y}_1 \\
                        \overline{y}_2 \\
                        \frac{S_{x_1y_1 + S{x_2y_2}}}{S_{x_1x_1}+S{x_2x_2}}
                        \end{pmatrix}
\end{align*}

95% confidence interval for $\alpha_1 - \alpha_2 + \beta(\overline{x}_2 - \overline{x}_1)$

\begin{align*}
\pmb{b} = \begin{pmatrix}
        1 \\
       -1 \\
      \overline{x}_2 - \overline{x}_1 
    \end{pmatrix} \\
\pmb{b}^T\pmb{\overline{\beta}} \pm t(n_1 + n_2 -3; 0.975) \sqrt{\frac{RSS}{n1+n2-3}\pmb{b}^T(\pmb{X}^T\pmb{X})^{-1} \pmb{b}}
\end{align*}

\begin{align*}
(\pmb{X}^T\pmb{X})^{-1} = \begin{pmatrix}
                          \frac{1}{n_1} & 0             & 0 \\
                            0           & \frac{1}{n_2} & 0 \\
                            0           & 0             & \frac{1}{S_{x_1x_1}+ S_{x_2x_2}}
                          \end{pmatrix}
\end{align*}


## ANOVA table

\tablehead{
\toprule
Component&df&SS&MS&F value\\
\midrule
}
\begin{xtabular}{lllll}
%\toprule
%Symbol&Description&Unit\\
%\midrule
Model       & $p-1$              & MSS            &$\frac{MSS}{p-1}$    &$\frac{\frac{MSS}{p-1}}{\frac{RSS}{n-p}}$ \\
Residual    & $n-p$              & RSS            &$\frac{RSS}{n-p}$    &     -                                  \\
Total       & $n-1$              & TSS            &   -                 &     -                                 
\end{xtabular}

* df : degrees of freedom
* SS : Sum of Squares
* MS : Mean Squares

F statistic ($H_0$ : all parameters = 0)
(large F values $\implies$ rejection of $H_0$)


== Difference between RSS and RSE ? 
== expanded formulas for $\sigma^2$ ?
== Prediction intervals vs confidence intervals
==paralell model
== model conversion






